{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7cb4ee",
   "metadata": {},
   "source": [
    "\n",
    "What is driving the errors in the Zestimates?\n",
    "\n",
    "Repository Format\n",
    "\n",
    "README.md:\n",
    "\n",
    "Contains a full outline of the project as well as information regarding the format of the repository and instructions for reproducing the results.\n",
    "\n",
    "Final_Report.ipynb: \n",
    "\n",
    "The final report containing a high level overview of the project including key takeaways, final results, and a recommendations.\n",
    "\n",
    "wrangle.py: \n",
    "\n",
    "Contains all code utilized for acquiring and preparing the Zillow property data for exploration and modeling.\n",
    "Contains the step by step acquisition and preparation process with details and explanations.\n",
    "\n",
    "explore.py:\n",
    "\n",
    "Contains functions used in the final report for producing visualizations and statistical test results of key takeaways from exploration.\n",
    "Contains the step by step exploratory analysis process with details and explanations.\n",
    "\n",
    "model.py: \n",
    "\n",
    "Contains functions used in the final report for producing and evaluating machine learning models.\n",
    "Contains functions used for building cluster models.\n",
    "Contains a Model class used for keeping track of features, target, hyperparameters, and the model used.\n",
    "Contains the step by step modeling process with details and explanations.\n",
    "\n",
    "\n",
    "Table of Contents\n",
    "Project Goals\n",
    "Project Description\n",
    "Initial Questions\n",
    "Data Dictionary\n",
    "Steps to reproduce\n",
    "Project Planning\n",
    "Data Acquisition\n",
    "Data Preparation\n",
    "Exploratory Analysis\n",
    "Modeling\n",
    "\n",
    "\n",
    "Project Goals\n",
    "\n",
    "The goal of this project is to identify key drivers of logerror in the Zillow property value estimates and come up with a model for predicting the logerror.\n",
    "\n",
    "Project Description\n",
    "\n",
    "To identify key attributes that drive the logerror in Zestimates. This will help us come up with better prediction models so that Zillow can remain competitive in the housing market. Compare all models and evaluated by how well they performs over the baseline. To give recommendations that can help reduce the overall average logerror.\n",
    "\n",
    "Initial Questions/hypotheses\n",
    "\n",
    "Does the county a property is located in affect it's log error?\n",
    "Does the tax_value of a house affect the logerror?\n",
    "Does the ratio of home sqft to lot sqft affect logerror?\n",
    "Does the year a house was built affect logerror?\n",
    "\n",
    "\n",
    "Data Dictionary\n",
    "\n",
    "Column               \t- Description\n",
    "\n",
    "\n",
    "lot_size\t            - Area of the lot in square feet\n",
    "\n",
    "\n",
    "yearbuilt\t            - The year the principal residence was built\n",
    "\n",
    "\n",
    "logerror\t            - The difference between the log of the Zestimate and the log of the sale price\n",
    "\n",
    "\n",
    "tax_value\t            - The total tax assessed value of the parcel\n",
    "\n",
    "\n",
    "square_feet\t            - Calculated total finished living area of the home\n",
    "\n",
    "\n",
    "regionidzip/zip_code\t- Zip code in which the property is located\n",
    "\n",
    "\n",
    "\n",
    "bathroomcnt\t            - Number of bathrooms in home including fractional bathrooms\n",
    "\n",
    "\n",
    "\n",
    "bedroomcnt\t            - Number of bedrooms in home\n",
    "\n",
    "\n",
    "Steps to reproduce\n",
    "\n",
    "Clone this repository into your local machine using the following command:\n",
    "git clone git@github.com:vincent-banuelos-patrick-amwayi/patrick_repository.git\n",
    "You will need Pandas, Numpy, Matplotlib, Seaborn, and SKLearn installed on your machine.\n",
    "If you don't have login credentials for the MySQL database hosted at data.codeup.com acquire login credentials.\n",
    "Create a file in the main directory titled \"env.py\" and put your login credentials in the following format:\n",
    "username = \"your_username\"\n",
    "password = \"your_password\"\n",
    "hostname = \"data.codeup.com\"\n",
    "Now you can start a Jupyter Notebook session and execute the code blocks in the Final_Report.ipynb notebook.\n",
    "\n",
    "Project Planning\n",
    "\n",
    "Data Acquisition\n",
    "\n",
    "We acquired data from the 2017 properties and predictions data for single unit / single family homes. The data is acessed from data.codeup.com using a SQL query. Approximately 52,000 observations are retrieved.\n",
    "\n",
    "The wrangle.ipynb: \n",
    "\n",
    "contains a reproducible step by step process for acquiring the data with details and explanations.\n",
    "\n",
    "explore.py: \n",
    "\n",
    "Contains the step by step exploratory analysis process with details and explanations.\n",
    "\n",
    "model.py: \n",
    "\n",
    "Contains the step by step modeling process with details and explanations.\n",
    "\n",
    "\n",
    "Steps Taken:\n",
    "\n",
    "Data Acquisition and Preparation\n",
    "\n",
    "We acquired data from the 2017 properties and predictions data for single unit / single family homes. The data is acessed from data.codeup.com using a SQL query. Approximately 52,000 observations are retrieved.\n",
    "\n",
    "Missing Values: On our initial analysis of the data shows a large amount of missing values. We kept the preparation simple by removing outliers, filling nulls for multiple columns and dropping those that were insignificant. However, in order to prevent a large loss of data we took the following steps:\n",
    "\n",
    "Remove any columns that are missing by .75.\n",
    "Remove any rows with a missing value by .75.\n",
    "Scaling\n",
    "This results in a train data of approximately 24,000 observations, reduced features which was deemed acceptable since we still had some data to work on.\n",
    "\n",
    "Scaling\n",
    "\n",
    "Steps Taken:\n",
    "\n",
    "Scale all datasets and remove outliers from train.\n",
    "Establish a baseline model\n",
    "Produce four models without using the cluster feature.\n",
    "Produce four models with the cluster feature.\n",
    "Analyze the results of each model on train and validate.\n",
    "\n",
    "\n",
    "\n",
    "Exploratory Analysis\n",
    "\n",
    "Steps Taken:\n",
    "\n",
    "Before exploration, dataset is split into a train set, validate set, and a test set to maintain the integrity of our process by keeping some data as unseen.\n",
    "At this phase we will analyze data in order to determine which features are most relevant for predicting logerror.\n",
    "\n",
    "We then analyze the distributions of values for each feature. Then we analyze the relation between each feature and the target variable logerror. Finally, we look at how various features interact with each other to determine if any new features can be engineered from existing ones to provide better insights.\n",
    "\n",
    "Clustering\n",
    "\n",
    "Steps Taken:\n",
    "\n",
    "Observe the interactions of multiple variables to identify which features may be useful for clustering.\n",
    "Build a clustering model and use it to create a new feature.\n",
    "Observe the results of the clustering model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fbc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
